<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>VR/AR Teleoperation Demos</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <h1>ðŸ“º VR/AR Teleoperation Demos</h1>

  <div class="video-grid">
    <div class="video-item">
      <h3>Data Collection: Dexterous Operation</h3>
      <iframe width="50%" height="315"
        src="https://www.youtube.com/embed/6bsc-GEpzrE"
        frameborder="0" allowfullscreen>
      </iframe>
      <p class="video-desc">
        The training data collection platform is designed for deformable object manipulation learning using visuo-tactile feedback. The task involves inserting a wire into an L-shaped pipe and guiding it to exit from the other end. A fiberscope is positioned at the pipeâ€™s exit to automatically detect successful wire traversal. Throughout the process, the wireâ€™s pose inside the pipe is recorded as an intermediate state, providing valuable data for learning-based manipulation models.
      </p>
    </div>

    <div class="video-item">
      <h3>Action Generation: Dexterous Operation</h3>
      <iframe width="50%" height="315"
        src="https://www.youtube.com/embed/dFMBLx5_5qg"
        frameborder="0" allowfullscreen>
      </iframe>
      <p class="video-desc">
        The training data collection platform is designed for deformable object manipulation learning using visuo-tactile feedback. The task involves inserting a wire into an L-shaped pipe and guiding it to exit from the other end. A fiberscope is positioned at the pipeâ€™s exit to automatically detect successful wire traversal. Throughout the process, the wireâ€™s pose inside the pipe is recorded as an intermediate state, providing valuable data for learning-based manipulation models.
      </p>
    </div>

    <div class="video-item">
      <h3>EMG for Adaptive Grasping Force</h3>
      <iframe width="50%" height="315"
        src="https://www.youtube.com/embed/tEekbPx2XkA"
        frameborder="0" allowfullscreen>
      </iframe>
      <p class="video-desc">
        Dual-loop system that use user's forearm EMG signal to control the grasping force of the robot hand. The real grasping effect is detected by the DIGIT visuo-tactile sensors mounted on robot hand fingertips and transferred back to user through the haptic vest.
      </p>
    </div>

    <div class="video-item">
      <h3>AR-based Dual Arm-Hand Control</h3>
      <iframe width="50%" height="315"
        src="https://www.youtube.com/embed/y_6gN3BuagU"
        frameborder="0" allowfullscreen>
      </iframe>
      <p class="video-desc">
        Two HapticX devices are mounted on user hands. The motion of user hands is tracked to control the robot arms with Hololens2 goggle detecting fingertip poses for Allegrohand control. The external force of robot arms are transferred back to HapticX as user haptic feedback.
      </p>
    </div>

    <div class="video-item">
      <h3>Virtual-Real Tacile Feedback</h3>
      <iframe width="50%" height="315"
        src="https://www.youtube.com/embed/A-qZSo430fw"
        frameborder="0" allowfullscreen>
      </iframe>
      <p class="video-desc">
        This video showcases a mixed-reality interaction system, which allows users to experience tactile sensations while interacting with virtual objects in a VR environment. A user is seen wearing a head-mounted display (HoloLens) and engaging with various virtual surfaces and geometries. Real-time fingertip tracking and visual-tactile feedback are demonstrated, emphasizing the system's capability to render physical sensations that correspond to virtual contact.      
      </p>
    </div>

    <div class="video-item">
      <h3>Eye-Tracking Control</h3>
      <iframe width="50%" height="315"
        src="https://www.youtube.com/embed/lyqS-4Wn6cM"
        frameborder="0" allowfullscreen>
      </iframe>
      <p class="video-desc">
        Use AR Hololens to get the user's eyetracking data and calibrate with an Azure Kinect RGBD camera to get the location of user focusing object in Camera coordinate. Use SAM (Segment Anything model) to segment all the foreground object and align with the user focusing object. When matched, the franka arm will move to the object and grasp it. The system support eye-tracking robot arm control.
      </p>
    </div>

    <div class="video-item">
      <h3>AR-based Robot Arm Teleoperation</h3>
      <iframe width="50%" height="315"
        src="https://www.youtube.com/embed/r2VvLG0dd9k"
        frameborder="0" allowfullscreen>
      </iframe>
      <p class="video-desc">
        Hololens2-based robot arm end effector control including grasping.
      </p>
    </div>

    <div class="video-item">
      <h3>AR-based Robot Dog Teleoperation</h3>
      <iframe width="50%" height="315"
        src="https://www.youtube.com/embed/MADtuJc3aHQ"
        frameborder="0" allowfullscreen>
      </iframe>
      <p class="video-desc">
        The user wears a Hololens2 goggle which is used to track hand gesture. The gesture is then transferred to control command and send to the high-level control system of the B1 robot dog and instruct its motion.
      </p>
    </div>


  </div>

  <footer>
    <p>Â© 2025 by Your Name</p>
  </footer>
</body>
</html>